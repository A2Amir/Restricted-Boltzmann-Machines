{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png\" width = 300, align = \"center\"></a>\n",
    "<h1 align=center><font size = 5>Deep Belief Network </font></h1>\n",
    "\n",
    "One problem with traditional multilayer perceptrons/artificial neural networks is that backpropagation can often lead to “local minima”. This is when your “error surface” contains multiple grooves and you fall into a groove that is not lowest possible groove as you perform gradient descent.\n",
    "\n",
    "__Deep belief networks__ solve this problem by using an extra step called __pre-training__. Pre-training is done before backpropagation and can lead to an error rate not far from optimal. This puts us in the “neighborhood” of the final solution. Then we use backpropagation to slowly reduce the error rate from there.\n",
    "\n",
    "DBNs can be divided in two major parts. The first one are multiple layers of Restricted Boltzmann Machines (RBMs) to pre-train our network. The second one is a feed-forward backpropagation network, that will further refine the results from the RBM stack.\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/15y15xs7w72eer0on3gbi8zu6835imru.png\" alt=\"DBN Model\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "with urllib.request.urlopen(\"http://deeplearning.net/tutorial/code/utils.py\") as url:\n",
    "    response=url.read()\n",
    "target=open(\"utils.py\",\"w\")\n",
    "target.write(response.decode(\"utf-8\"))\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from  PIL  import Image\n",
    "from utils import tile_raster_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, input_size,output_size):\n",
    "        #Defining the hyperparameters\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.epochs=5\n",
    "        self.learning_rate=1.0\n",
    "        self.bachsize=100\n",
    "        \n",
    "        #Initializing weights and biases as matrices full of zeroes\n",
    "        self.w=np.zeros([input_size,output_size],np.float32)\n",
    "        self.hb=np.zeros([output_size],np.float32)\n",
    "        self.vb=np.zeros([input_size],np.float32)\n",
    "    \n",
    "    def prob_h_given_v(self,visible,w , hb):\n",
    "        return tf.nn.sigmoid(tf.matmul(visible,w)+hb)\n",
    "    \n",
    "    def prob_v_given_h(self,hidden,w,vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden,tf.transpose(w))+vb)\n",
    "    \n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs-tf.random_uniform(tf.shape(probs))))\n",
    "    \n",
    "    def train(self,X):\n",
    "        _w=tf.placeholder(tf.float32,[self.input_size,self.output_size])\n",
    "        _hb=tf.placeholder(tf.float32,[self.output_size])\n",
    "        _vb=tf.placeholder(tf.float32,[self.input_size])\n",
    "        \n",
    "        prv_w=np.zeros([self.input_size,self.output_size],np.float32)\n",
    "        prv_hb=np.zeros([self.output_size],np.float32)\n",
    "        prv_vb=np.zeros([self.input_size],np.float32)\n",
    "\n",
    "        \n",
    "        cur_w=np.zeros([self.input_size,self.output_size],np.float32)\n",
    "        cur_hb=np.zeros([self.output_size],np.float32)\n",
    "        cur_vb=np.zeros([self.input_size],np.float32)\n",
    "        \n",
    "        v0=tf.placeholder(tf.float32,[None,self.input_size])\n",
    "        \n",
    "        h0=self.sample_prob(self.prob_h_given_v(v0,_w,_hb))\n",
    "        v1=self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "        \n",
    "         #Create the Gradients\n",
    "        positive_grad=tf.matmul(tf.transpose(v0),h0)\n",
    "        negative_grad=tf.matmul(tf.transpose(v1),h1)\n",
    "        \n",
    "       #Update learning rates for the layers\n",
    "        update_w=_w+self.learning_rate*(positive_grad-negative_grad)/tf.to_float(tf.shape(v0)[0])\n",
    "        update_vb=_vb+self.learning_rate* tf.reduce_mean(v0 - v1, 0)\n",
    "        update_hb=_hb+self.learning_rate* tf.reduce_mean(h0 - h1, 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Find the error rate\n",
    "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
    "        \n",
    "        #Training loop\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #For each epoch\n",
    "            for epoch in range(self.epochs):\n",
    "                #For each step/batch\n",
    "                for start, end in zip(range(0, len(X), self.bachsize),range(self.bachsize,len(X), self.bachsize)):\n",
    "                    batch = X[start:end]\n",
    "                    #Update the rates\n",
    "                    cur_w = sess.run(update_w, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    prv_w = cur_w\n",
    "                    prv_hb = cur_hb\n",
    "                    prv_vb = cur_vb\n",
    "                error = sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb})\n",
    "                print ('Epoch: %d' % epoch,'reconstruction error: %f' % error)\n",
    "            self.w = prv_w\n",
    "            self.hb = prv_hb\n",
    "            self.vb = prv_vb\n",
    "\n",
    "    #Create expected output for our DBN\n",
    "    def rbm_outpt(self, X):\n",
    "        input_X = tf.constant(X)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        out = tf.nn.sigmoid(tf.matmul(input_X, _w) + _hb)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            return sess.run(out)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Getting the MNIST data provided by Tensorflow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#Loading in the mnist data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM:  0   784 -> 500\n",
      "RBM:  1   500 -> 200\n",
      "RBM:  2   200 -> 50\n"
     ]
    }
   ],
   "source": [
    "RBM_hidden_sizes = [500, 200 , 50 ] #create 2 layers of RBM with size 400 and 100\n",
    "\n",
    "#Since we are training, set input as training data\n",
    "inpX = trX\n",
    "\n",
    "#Create list to hold our RBMs\n",
    "rbm_list = []\n",
    "\n",
    "#Size of inputs is the number of inputs in the training set\n",
    "input_size = inpX.shape[1]\n",
    "\n",
    "#For each RBM we want to generate\n",
    "for i, size in enumerate(RBM_hidden_sizes):\n",
    "    print ('RBM: ',i,' ',input_size,'->', size)\n",
    "    rbm_list.append(RBM(input_size, size))\n",
    "    input_size = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each RBM in our list\n",
    "for rbm in rbm_list:\n",
    "    print ('New RBM:')\n",
    "    #Train a new one\n",
    "    rbm.train(inpX) \n",
    "    #Return the output layer\n",
    "    inpX = rbm.rbm_outpt(inpX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, sizes, X, Y):\n",
    "        #Initialize hyperparameters\n",
    "        self._sizes = sizes\n",
    "        self._X = X\n",
    "        self._Y = Y\n",
    "        self.w_list = []\n",
    "        self.b_list = []\n",
    "        self._learning_rate =  1.0\n",
    "        self._momentum = 0.0\n",
    "        self._epoches = 10\n",
    "        self._batchsize = 100\n",
    "        input_size = X.shape[1]\n",
    "        \n",
    "        #initialization loop\n",
    "        for size in self._sizes + [Y.shape[1]]:\n",
    "            #Define upper limit for the uniform distribution range\n",
    "            max_range = 4 * math.sqrt(6. / (input_size + size))\n",
    "            \n",
    "            #Initialize weights through a random uniform distribution\n",
    "            self.w_list.append(\n",
    "                np.random.uniform( -max_range, max_range, [input_size, size]).astype(np.float32))\n",
    "            \n",
    "            #Initialize bias as zeroes\n",
    "            self.b_list.append(np.zeros([size], np.float32))\n",
    "            input_size = size\n",
    "      \n",
    "    #load data from rbm\n",
    "    def load_from_rbms(self, dbn_sizes,rbm_list):\n",
    "        #Check if expected sizes are correct\n",
    "        assert len(dbn_sizes) == len(self._sizes)\n",
    "        \n",
    "        for i in range(len(self._sizes)):\n",
    "            #Check if for each RBN the expected sizes are correct\n",
    "            assert dbn_sizes[i] == self._sizes[i]\n",
    "        \n",
    "        #If everything is correct, bring over the weights and biases\n",
    "        for i in range(len(self._sizes)):\n",
    "            self.w_list[i] = rbm_list[i].w\n",
    "            self.b_list[i] = rbm_list[i].hb\n",
    "\n",
    "    #Training method\n",
    "    def train(self):\n",
    "        #Create placeholders for input, weights, biases, output\n",
    "        _a = [None] * (len(self._sizes) + 2)\n",
    "        _w = [None] * (len(self._sizes) + 1)\n",
    "        _b = [None] * (len(self._sizes) + 1)\n",
    "        _a[0] = tf.placeholder(\"float\", [None, self._X.shape[1]])\n",
    "        y = tf.placeholder(\"float\", [None, self._Y.shape[1]])\n",
    "        \n",
    "        #Define variables and activation functoin\n",
    "        for i in range(len(self._sizes) + 1):\n",
    "            _w[i] = tf.Variable(self.w_list[i])\n",
    "            _b[i] = tf.Variable(self.b_list[i])\n",
    "        for i in range(1, len(self._sizes) + 2):\n",
    "            _a[i] = tf.nn.sigmoid(tf.matmul(_a[i - 1], _w[i - 1]) + _b[i - 1])\n",
    "        \n",
    "        #Define the cost function\n",
    "        cost = tf.reduce_mean(tf.square(_a[-1] - y))\n",
    "        \n",
    "        #Define the training operation (Momentum Optimizer minimizing the Cost function)\n",
    "        train_op = tf.train.MomentumOptimizer(self._learning_rate, self._momentum).minimize(cost)\n",
    "        \n",
    "        #Prediction operation\n",
    "        predict_op = tf.argmax(_a[-1], 1)\n",
    "        \n",
    "        #Training Loop\n",
    "        with tf.Session() as sess:\n",
    "            #Initialize Variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            #For each epoch\n",
    "            for i in range(self._epoches):\n",
    "                \n",
    "                #For each step\n",
    "                for start, end in zip(range(0, len(self._X), self._batchsize), range(self._batchsize, len(self._X), self._batchsize)):\n",
    "                    \n",
    "                    #Run the training operation on the input data\n",
    "                    sess.run(train_op, feed_dict={ _a[0]: self._X[start:end], y: self._Y[start:end]})\n",
    "                \n",
    "                for j in range(len(self._sizes) + 1):\n",
    "                    #Retrieve weights and biases\n",
    "                    self.w_list[j] = sess.run(_w[j])\n",
    "                    self.b_list[j] = sess.run(_b[j])\n",
    "                \n",
    "                print (\"Accuracy rating for epoch \" + str(i) + \": \" + str(np.mean(np.argmax(self._Y, axis=1) == sess.run(predict_op, feed_dict={_a[0]: self._X, y: self._Y}))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rating for epoch 0: 0.10390909090909091\n",
      "Accuracy rating for epoch 1: 0.10390909090909091\n",
      "Accuracy rating for epoch 2: 0.10390909090909091\n",
      "Accuracy rating for epoch 3: 0.10390909090909091\n",
      "Accuracy rating for epoch 4: 0.10390909090909091\n",
      "Accuracy rating for epoch 5: 0.10390909090909091\n",
      "Accuracy rating for epoch 6: 0.10390909090909091\n",
      "Accuracy rating for epoch 7: 0.10390909090909091\n",
      "Accuracy rating for epoch 8: 0.10390909090909091\n",
      "Accuracy rating for epoch 9: 0.10390909090909091\n"
     ]
    }
   ],
   "source": [
    "nNet = NN(RBM_hidden_sizes, trX, trY)\n",
    "nNet.load_from_rbms(RBM_hidden_sizes,rbm_list)\n",
    "nNet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
